### ResNet
Originally we learn the weights that maps input x to output H(x), now if we define the output to be F(x) + x where input x is added as identity mapping, then the weights actually represent the residual F(x) = H(x) - x. This residual is easier to learn and find some small fluctuations in addition to identity.

Also, due to the skip/shortcut connections, gradients can pass through each residual block (by chain rule, d(x+a)/dx=1+da/dx, the "1" indicates that the gradient at the layer can directly pass to shallower layers), therefore less gradient vanishing is possible.

### Bottleneck block in ResNet
Bottleneck refers to layers that have dramatically different input and output channels, either high or low input/output channel ratio. Bottleneck block reduces the computational cost of convolution layer (1x1 filter has fewer parameters)
[zhihu](https://zhuanlan.zhihu.com/p/54289848)
