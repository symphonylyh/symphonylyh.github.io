
### Face verification vs. Face recognition
FV is 1:1 mapping, give an input image A & a person ID B, check if A is B.

FR is 1:N mapping, give an input image A & a database B, check if A is in B. For the checking step, faster classifier algorithms include SVM (SVM will directly tell you the distance rather than linear search the database) and k Nearest Neighbor.

The concept is the same.
face embedding network.

[1](https://www.learnopencv.com/face-recognition-an-introduction-for-beginners/) if we trained a face embedding network, how do we add new faces? do we need to retrain? No, just use the existing embedding network, generate the new face embedding and add to database.

### SVM
Find hyperplane to separate data from different classes.
The points closest to the Decision Boundary is called support vectors. SVM finds the DB by maximize its distance from the support vectors.

### Triplet loss
Used a lot in fine-grained identification problems to differentiate between data. Softmax can't handle when number of classes becomes over millions (last layer will be too large).

### N-shot learning
[1](https://blog.floydhub.com/n-shot-learning/)

### Optimizer
[1](https://ruder.io/optimizing-gradient-descent/index.html)

### Interrupt, exception

### Cache

### PyTorch
`model.eval()` affects the behavior of layers (especially Dropout & BatchNorm), but it has nothing to do with saving memory (i.e. all the gradients are still calculated)

`with torch.no_grad()` turns off the autograd engine, the gradients won't be calculated so memory is much less. `requires_grad = False` for all parameters has the same effect. This is the inference mode where you ensure `.backward()` will never be called

`Variable()` wraps a tensor and provide the autograd method (like .backward)
